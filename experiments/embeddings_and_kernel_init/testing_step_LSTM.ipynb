{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include project path to available custom class at jupyter\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/PycharmProjects/RecurrentNetworks/'))\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import datetime\n",
    "import re\n",
    "import copy\n",
    "K.set_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.definition_network as dn\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from network_model.model_class import ModelClass\n",
    "from utils.experiment_processes import ExperimentProcesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tests Step 1: (A) Kernel Functions and (B) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_BY_DISORDER = dict({'anxiety': {'total_registers': 1040, 'subdirectory': 'anxiety'},\n",
    "                           'depression': {'total_registers': 2160, 'subdirectory': 'depression'},\n",
    "                           'anxiety,depression': {'total_registers': 880, 'subdirectory': 'anxiety,depression'},\n",
    "                           'anx_dep_multilabel': {'total_registers': 2640, 'subdirectory': 'anx_dep_multilabel'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(dir_path):\n",
    "    ret = True\n",
    "    try:\n",
    "        os.makedirs(dir_path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % dir_path)\n",
    "        ret = False\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % dir_path)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dir_tree(test_dir_path, disorder_dir_path):    \n",
    "    if create_directory(dn.PATH_PROJECT + test_dir_path):\n",
    "        _ = create_directory(dn.PATH_PROJECT + test_dir_path + '/tokenizers/' + disorder_dir_path)\n",
    "        _ = create_directory(dn.PATH_PROJECT + test_dir_path + '/pre_train_embeddings/' + disorder_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_file(src_dir, dst_dir, extension_file_lst):\n",
    "    file_lst = [file_name for file_name in os.listdir(src_dir) \n",
    "                             for ext_file in extension_file_lst if file_name.endswith(ext_file)]    \n",
    "    \n",
    "    for file in file_lst:\n",
    "        shutil.move(src_dir + file, dst_dir + file)\n",
    "        print('Move file to %s' % (dst_dir + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_test_files(test_dir_path, disorder_dir_path):\n",
    "    # Move files test in root directory\n",
    "    move_file(dn.PATH_PROJECT, dn.PATH_PROJECT + test_dir_path + '/', [\".h5\", \".df\", \".csv\", \".txt\"])\n",
    "\n",
    "    # Move tokenizer\n",
    "    move_file(dn.PATH_PROJECT + 'tokenizers/' + disorder_dir_path  + '/', \n",
    "              dn.PATH_PROJECT + test_dir_path + '/tokenizers/' + disorder_dir_path  + '/', [\".df\"])\n",
    "    \n",
    "    # Move pre_train_embeddings\n",
    "    move_file(dn.PATH_PROJECT + 'pre_train_embeddings/' + disorder_dir_path  + '/', \n",
    "              dn.PATH_PROJECT + test_dir_path + '/pre_train_embeddings/' + disorder_dir_path  + '/', [\".df\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(set_params):\n",
    "    exp = ExperimentProcesses(set_params['log_file'])\n",
    "    exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                                   total_registers=PARAMS_BY_DISORDER[set_params['disorder']]['total_registers'], \n",
    "                                   subdirectory=PARAMS_BY_DISORDER[set_params['disorder']]['subdirectory'])\n",
    "\n",
    "    exp.pp_data.vocabulary_size = 5000\n",
    "    exp.pp_data.embedding_size = 300\n",
    "    exp.pp_data.max_posts = 1750\n",
    "    exp.pp_data.max_terms_by_post = 300\n",
    "    exp.pp_data.format_input_data = dn.InputData.POSTS_ONLY_TEXT\n",
    "    exp.pp_data.remove_stopwords = False\n",
    "    exp.pp_data.delete_low_tfid = False\n",
    "    exp.pp_data.min_df = 0\n",
    "    exp.pp_data.min_tf = 0\n",
    "    exp.pp_data.random_posts = False\n",
    "    exp.pp_data.random_users = False\n",
    "    exp.pp_data.tokenizing_type = 'WE'\n",
    "    exp.pp_data.type_prediction_label = set_params['type_prediction_label']\n",
    "\n",
    "    exp.use_custom_metrics = False\n",
    "    exp.use_valid_set_for_train = True\n",
    "    exp.valid_split_from_train_set = 0.0\n",
    "    exp.imbalanced_classes = False\n",
    "    exp.pp_data.embedding_type = set_params['embedding_type']\n",
    "    exp.pp_data.word_embedding_custom_file = set_params['embedding_custom_file']\n",
    "    exp.pp_data.use_embedding = set_params['use_embedding']\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "\n",
    "    model_cs = ModelClass(1)\n",
    "    model_cs.loss_function = 'binary_crossentropy'\n",
    "    model_cs.optmizer_function = dn.OPTIMIZER_FUNCTIONS[set_params['optimizer_function']]\n",
    "    model_cs.epochs = set_params['epochs']\n",
    "    model_cs.batch_size = set_params['batch_size']\n",
    "    model_cs.patience_train = set_params['epochs'] / 2\n",
    "\n",
    "    model_cs.use_embedding_pre_train = exp.pp_data.use_embedding\n",
    "    model_cs.embed_trainable = (model_cs.use_embedding_pre_train == \\\n",
    "                                (dn.UseEmbedding.RAND or dn.UseEmbedding.NON_STATIC))\n",
    "\n",
    "    emb_name = set_params['function']\n",
    "    if set_params['embedding_custom_file'] != '':\n",
    "        emb_name = exp.pp_data.word_embedding_custom_file.split('.')[0]\n",
    "\n",
    "    we_file_name = 'ET_' + str(exp.pp_data.embedding_type.value) + '_UE_' + str(exp.pp_data.use_embedding.value) +\\\n",
    "                   '_EF_' + emb_name + '_' + set_params['kernel_function'].split('_')[0]\n",
    "\n",
    "    exp.experiment_name = set_params['name_model'] + '_lstm_exp9_var_L3' + '_N' +\\\n",
    "                          str(set_params['neuronios_by_layer']) + '_B' + str(set_params['batch_size']) + '_E' +\\\n",
    "                          str(set_params['epochs']) + '_D' + str(set_params['dropouts']) + '_OF' +\\\n",
    "                          model_cs.optmizer_function + '_HL' + str(set_params['hidden_layer']) + '_' + we_file_name\n",
    "    \n",
    "    #TRAIN\n",
    "    np.random.seed(dn.SEED)\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "    x_train, y_train, x_valid, y_valid, num_words, embedding_matrix = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "    \n",
    "    model_cs.model = Sequential()\n",
    "    model_cs.model.add(Embedding(exp.pp_data.vocabulary_size, exp.pp_data.embedding_size,\n",
    "                                 trainable=model_cs.embed_trainable, name='emb_' + set_params['name_model']))\n",
    "    \n",
    "    for id_hl in range(set_params['hidden_layer']-1):\n",
    "        model_cs.model.add(LSTM(set_params['neuronios_by_layer'], activation=set_params['activation'],\n",
    "                                dropout=set_params['dropouts'], recurrent_dropout=set_params['dropouts'],\n",
    "                                kernel_initializer= set_params['kernel_function'],\n",
    "                                return_sequences=set_params['return_sequences'],\n",
    "                                name='dense_'+str(id_hl)+'_' + set_params['name_model']))\n",
    "\n",
    "    model_cs.model.add(LSTM(set_params['neuronios_by_layer'], activation=set_params['activation'], \n",
    "                            dropout=set_params['dropouts'], recurrent_dropout=set_params['dropouts'],\n",
    "                            kernel_initializer= set_params['kernel_function'],\n",
    "                            name='dense_'+str(id_hl+1)+'_' + set_params['name_model']))\n",
    "\n",
    "    model_cs.model.add(Dense(set_params['neuronios_dense_layer'], \n",
    "                             kernel_initializer=set_params['kernel_function'],\n",
    "                             activation=set_params['act_last_layer'],\n",
    "                             name='dense_'+str(id_hl+2)+'_' + set_params['name_model']))\n",
    "\n",
    "#     print(model_cs.model.summary())\n",
    "    time_ini_exp = datetime.datetime.now()\n",
    "    exp.generate_model_hypeparams(model_cs, x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
    "    exp.set_period_time_end(time_ini_exp, 'Total experiment')\n",
    "\n",
    "    del x_train, y_train, x_valid, y_valid, num_words, embedding_matrix\n",
    "    \n",
    "    #TEST\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TEST_DATA_MODEL\n",
    "    np.random.seed(dn.SEED)\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "    x_test, y_test = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "\n",
    "    model_cs.model = exp.load_model(dn.PATH_PROJECT + exp.experiment_name + '.h5')\n",
    "    exp.save_geral_configs('Experiment Specific Configuration: ' + exp.experiment_name)\n",
    "    exp.save_summary_model(model_cs.model)\n",
    "    exp.predict_samples(model_cs, x_test, y_test)\n",
    "\n",
    "    del x_test, y_test, model_cs, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Test 1A - Comparing model performance using Glorot x LeCun Uniform Kernel Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Definitions for test scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kernel_function(id_test, kernel_function, prefix_name, disorder):\n",
    "    set_params = dict({'disorder': disorder,\n",
    "                       'log_file': prefix_name+'_lstm_L3',\n",
    "                       'name_model': prefix_name,\n",
    "                       'function': 'glove6B300d',\n",
    "                       'type_prediction_label': dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL,\n",
    "                       'embedding_type': dn.EmbeddingType.GLOVE_6B,\n",
    "                       'embedding_custom_file': '',\n",
    "                       'use_embedding': dn.UseEmbedding.STATIC,\n",
    "                       'optimizer_function': dn.OptimizerFunction.ADAM.value,\n",
    "                       'hidden_layer': 3,\n",
    "                       'neuronios_by_layer': 16,\n",
    "                       'activation': 'tanh',\n",
    "                       'return_sequences': True,\n",
    "                       'dropouts': 0.2,\n",
    "                       'kernel_function': kernel_function,\n",
    "                       'neuronios_dense_layer': 3,\n",
    "                       'act_last_layer': 'sigmoid',\n",
    "                       'batch_size': 40,\n",
    "                       'epochs': 32})\n",
    "\n",
    "    generate_model(set_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run Test Kernel Function Variations to Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disorders_dct = dict({'anxiety': 'a', 'depression': 'd', 'anxiety,depression': 'ad'})\n",
    "for disorder, prefix in disorders_dct.items():\n",
    "    for kernel_function in ['glorot_uniform', 'lecun_uniform']:\n",
    "        for id_test in range(1,11):\n",
    "            name_test = 'test_kf_'+kernel_function[0:2]+'_'+prefix+str(id_test)\n",
    "\n",
    "            print(name_test)\n",
    "            create_test_dir_tree(name_test, disorder)\n",
    "            \n",
    "            test_kernel_function(id_test, kernel_function, name_test, disorder)\n",
    "            \n",
    "            move_test_files(name_test, disorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Test 1B - Comparing model performance using differents Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Auxiliar Function and Definitions for test scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emb_test_name(pfx_emb, word_embedding_custom_file):\n",
    "    prefix = pfx_emb.split('_')[0]\n",
    "    function = pfx_emb.split('_')[1]\n",
    "\n",
    "    we_file_substr = 'None'\n",
    "\n",
    "    if len(word_embedding_custom_file) > 0:\n",
    "        we_file_substr = word_embedding_custom_file.split('-')\n",
    "        \n",
    "        if len(we_file_substr[2]) == 1:\n",
    "            we_file_substr = we_file_substr[1][0:4]+'_'+we_file_substr[4]\n",
    "        else:\n",
    "            we_file_substr = we_file_substr[1][0:4]+'_'+we_file_substr[2]\n",
    "\n",
    "    return prefix, function, we_file_substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding_performance(id_test, set_params, prefix_name, disorder):\n",
    "    set_params.update({'disorder': disorder,\n",
    "                       'log_file': prefix_name+'_lstm_L3',\n",
    "                       'name_model': prefix_name,\n",
    "                       'optimizer_function': dn.OptimizerFunction.ADAM.value,\n",
    "                       'hidden_layer': 3,\n",
    "                       'neuronios_by_layer': 16,\n",
    "                       'activation': 'tanh',\n",
    "                       'return_sequences': True,\n",
    "                       'dropouts': 0.2,\n",
    "                       'kernel_function': 'glorot_uniform',\n",
    "                       'neuronios_dense_layer': 3,\n",
    "                       'act_last_layer': 'sigmoid',\n",
    "                       'batch_size': 40,\n",
    "                       'epochs': 32})\n",
    "\n",
    "    generate_model(set_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Run Test Word Embeddings Variations to Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disorders_dct = dict({'anxiety': 'a', 'depression': 'd', 'anxiety,depression': 'ad'})\n",
    "\n",
    "embedding_types = dict({'g6_glove6B300d': dn.EmbeddingType.GLOVE_6B,\n",
    "                        'gt_gloveTwitter': dn.EmbeddingType.GLOVE_TWITTER,\n",
    "                        'gn_googleNews': dn.EmbeddingType.WORD2VEC,\n",
    "                        'wc_w2vCustom': dn.EmbeddingType.WORD2VEC_CUSTOM,\n",
    "                        'gc_gloveCustom': dn.EmbeddingType.GLOVE_CUSTOM})\n",
    "\n",
    "use_embeddings = [dn.UseEmbedding.STATIC, dn.UseEmbedding.NON_STATIC]\n",
    "\n",
    "for disorder, prefix_disorder in disorders_dct.items():\n",
    "    for pfx_emb, embedding_type in embedding_types.items():\n",
    "        if embedding_type == dn.EmbeddingType.WORD2VEC_CUSTOM:\n",
    "            word_embedding_custom_files = ['SMHD-Skipgram-AllUsers-300.bin', 'SMHD-CBOW-AllUsers-300.bin',\n",
    "                                           'SMHD-Skipgram-A-D-ADUsers-300.bin', 'SMHD-CBOW-A-D-ADUsers-300.bin']\n",
    "        elif embedding_type == dn.EmbeddingType.GLOVE_CUSTOM:\n",
    "            word_embedding_custom_files = ['SMHD-glove-AllUsers-300.pkl', 'SMHD-glove-A-D-ADUsers-300.pkl']\n",
    "        else:\n",
    "            word_embedding_custom_files = ['']\n",
    "\n",
    "        for word_embedding_custom_file in word_embedding_custom_files:\n",
    "            for use_embedding in use_embeddings:\n",
    "                for id_test in range(1,11):\n",
    "                    prefix, function, we_file_substr = build_emb_test_name(pfx_emb, word_embedding_custom_file)\n",
    "\n",
    "                    name_test = 'test_'+prefix_disorder+str(id_test)+'_'+prefix+'_UE_'+str(use_embedding.value)+\\\n",
    "                                '_WF_'+we_file_substr\n",
    "                                 \n",
    "                    print(name_test)\n",
    "                    create_test_dir_tree(name_test, disorder)\n",
    "                    \n",
    "                    set_params = dict({'function': function,\n",
    "                                       'type_prediction_label': dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL,\n",
    "                                       'embedding_type': embedding_type,\n",
    "                                       'embedding_custom_file': word_embedding_custom_file,\n",
    "                                       'use_embedding': use_embedding})\n",
    "\n",
    "                    test_embedding_performance(id_test, set_params, name_test, disorder)\n",
    "                    move_test_files(name_test, disorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
